{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to be applied to each image\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "     transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # Normalize the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('<path to evaluation csv file>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_resnet = torchvision.datasets.ImageFolder(root='<path to resnet dataset folder>', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet_model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, len(dataset_resnet.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resnet_model.load_state_dict(torch.load('<path to saved trained resnet weigths eg:resnet1876.17260787992495.pth>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df['images'].tolist()\n",
    "class_list = df['classes'].tolist()\n",
    "correct = 0\n",
    "wrong = []\n",
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_a = '<path to pokemon-a dataset>'\n",
    "pokemon_b = '<path to pokemon-b dataset>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokedex = pd.read_csv('<path to pokedex.csv>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farfetchd Farfetch'd 83.png\n",
      "Charmander Eevee 133.png\n",
      "MrMime Mr. Mime 122.png\n",
      "Nidorino Nidoran♂ 32.png\n",
      "Mewtwo Mew 151.png\n",
      "Nidorina Nidoran♀ 29.png\n"
     ]
    }
   ],
   "source": [
    "a_images = os.listdir(pokemon_a)\n",
    "\n",
    "for img in a_images:\n",
    "    img_path = pokemon_a + img\n",
    "    index = img.split('.')[0]\n",
    "    if \"-\" in index:\n",
    "        index = index.split('-')[0]\n",
    "    correct_label = list(pokedex[pokedex['Name'] == int(index)]['Type 1'])[0]\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "    image =  transform(image)\n",
    "    image =  image.view(1, 3, 224, 224)\n",
    "    resnet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_resnet_label = resnet_model(image)\n",
    "        predicted_resnet_label = torch.argmax(predicted_resnet_label)\n",
    "        predicted_class = dataset_resnet.classes[predicted_resnet_label]\n",
    "    if correct_label.lower() == predicted_class.lower():\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(predicted_class, correct_label, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9883040935672515\n"
     ]
    }
   ],
   "source": [
    "print((correct)/len(a_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pidgeot Pidgeotto 17.jpg\n",
      "Weepinbell Victreebel 71.jpg\n",
      "Muk Grimer 88.jpg\n",
      "Nidorino Nidoran♂ 32.jpg\n",
      "Farfetchd Farfetch'd 83.jpg\n",
      "MrMime Mr. Mime 122.jpg\n",
      "Bulbasaur Ivysaur 2.jpg\n",
      "Pidgeotto Pidgey 16.jpg\n",
      "Grimer Muk 89.jpg\n",
      "Mankey Primeape 57.jpg\n",
      "Nidorina Nidoran♀ 29.jpg\n",
      "Rhydon Charizard 6-mega-x.jpg\n",
      "1.894736842105263\n"
     ]
    }
   ],
   "source": [
    "b_images = os.listdir(pokemon_b)\n",
    "\n",
    "for img in b_images:\n",
    "    img_path = pokemon_b + img\n",
    "    index = img.split('.')[0]\n",
    "    if \"-\" in index:\n",
    "        index = index.split('-')[0]\n",
    "    correct_label = list(pokedex[pokedex['Name'] == int(index)]['Type 1'])[0]\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "    image =  transform(image)\n",
    "    image =  image.view(1, 3, 224, 224)\n",
    "    resnet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_resnet_label = resnet_model(image)\n",
    "        predicted_resnet_label = torch.argmax(predicted_resnet_label)\n",
    "        predicted_class = dataset_resnet.classes[predicted_resnet_label]\n",
    "    if correct_label.lower() == predicted_class.lower():\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(predicted_class, correct_label, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "print((correct)/(len(b_images)+len(a_images)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "de935e14300db630c8585086d88fa64e33aea992cc54f5441c580cc68c8bfe48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
