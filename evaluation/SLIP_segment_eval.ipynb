{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.datasets import ImageFolder\n",
    "import albumentations as A\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from segment_anything import build_sam, SamAutomaticMaskGenerator, sam_model_registry\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your dataset folder\n",
    "dataset_path = '<path to evaluation dataset folder path>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('<path to evaluation csv file>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (24): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (25): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (26): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (27): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (28): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (29): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (30): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (31): Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0): Embedding(1, 256)\n",
       "      (1): Embedding(1, 256)\n",
       "      (2): Embedding(1, 256)\n",
       "      (3): Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0): TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_checkpoint = \"<path to SAM weights: sam_vit_h_4b8939.pth>\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights to load them here\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "        model = sam,\n",
    "        points_per_side = 16,\n",
    "        pred_iou_thresh =  0.9,\n",
    "        min_mask_region_area = 1,\n",
    "        crop_overlap_ratio = 0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box_xywh_to_xyxy(box):\n",
    "    \"\"\"\n",
    "    Convert bounding box from [x, y, w, h] format to [x1, y1, x2, y2] format.\n",
    "\n",
    "    Args:\n",
    "        box (list): Bounding box coordinates in [x, y, w, h] format.\n",
    "\n",
    "    Returns:\n",
    "        list: Bounding box coordinates in [x1, y1, x2, y2] format.\n",
    "    \"\"\"\n",
    "    x1 = box[0]\n",
    "    y1 = box[1]\n",
    "    x2 = box[0] + box[2]\n",
    "    y2 = box[1] + box[3]\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, segmentation_mask):\n",
    "    \"\"\"\n",
    "    Segment an image using a binary segmentation mask.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image.\n",
    "        segmentation_mask (ndarray): Binary segmentation mask.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image.Image: Segmented image with the background set to black.\n",
    "    \"\"\"\n",
    "    image_array = np.array(image)\n",
    "    segmented_image_array = np.zeros_like(image_array)\n",
    "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
    "    segmented_image = Image.fromarray(segmented_image_array)\n",
    "    black_image = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
    "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
    "    transparency_mask[segmentation_mask] = 255\n",
    "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
    "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
    "    return black_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations(annotations):\n",
    "    \"\"\"\n",
    "    Filter a list of annotations based on area and segmentation overlap.\n",
    "\n",
    "    Args:\n",
    "        annotations (list): List of annotation dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list of annotations.\n",
    "    \"\"\"\n",
    "    annotations.sort(key=lambda x: x['area'], reverse=True)\n",
    "    to_remove = set()\n",
    "    for i in range(0, len(annotations)):\n",
    "        a = annotations[i]\n",
    "        for j in range(i + 1, len(annotations)):\n",
    "            b = annotations[j]\n",
    "            if i != j and j not in to_remove:\n",
    "                # check if \n",
    "                if b['area'] < a['area']:\n",
    "                    if (a['segmentation'] & b['segmentation']).sum() / b['segmentation'].sum() > 0.8:\n",
    "                        to_remove.add(j)\n",
    "    return [a for i, a in enumerate(annotations) if i not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    image_path = \"<path to evaluation images folder>\"\n",
    "    captions_path = \"<path to captions.csv folder>\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    text_tokenizer = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        Initializes a CLIPDataset object.\n",
    "\n",
    "        Args:\n",
    "            image_filenames (list): List of image filenames.\n",
    "            captions (list): List of captions corresponding to the images.\n",
    "            tokenizer (transformers.tokenization_utils_base.PreTrainedTokenizer):\n",
    "                Tokenizer object used to tokenize the captions.\n",
    "            transforms (torchvision.transforms.Compose): Image transformations to be applied.\n",
    "\n",
    "        Note:\n",
    "            - `image_filenames` and `captions` must have the same length.\n",
    "              If there are multiple captions for each image, the `image_filenames` list\n",
    "              must have repetitive file names.\n",
    "\n",
    "        \"\"\"\n",
    "        self.image_filenames = image_filenames # one description per image\n",
    "        self.captions = list(captions) # one description per image\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=200\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the encoded caption, image tensor,\n",
    "                    and the original caption.\n",
    "\n",
    "        \"\"\"\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        item['image'] = [1]\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed-size vector.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the image encoder model. Default is 'resnet50'.\n",
    "        pretrained (bool): Whether to use pretrained weights for the model. Default is True.\n",
    "        trainable (bool): Whether to make the model trainable. Default is True.\n",
    "\n",
    "    Attributes:\n",
    "        model (torchvision.models.ResNet): Image encoder model.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name='resnet50', pretrained=True, trainable=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes an ImageEncoder object.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the image encoder model. Default is 'resnet50'.\n",
    "            pretrained (bool): Whether to use pretrained weights for the model. Default is True.\n",
    "            trainable (bool): Whether to make the model trainable. Default is True.\n",
    "\n",
    "        \"\"\"    \n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ImageEncoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded image representation.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    \"\"\"\n",
    "    Create train and validation dataframes from a captions CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Train dataframe containing a subset of the captions.\n",
    "        pandas.DataFrame: Validation dataframe containing a subset of the captions.\n",
    "\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(f\"{config.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    \"\"\"\n",
    "    Build data loaders for training or validation using the given dataframe and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing image and caption data.\n",
    "        tokenizer: Tokenizer object for encoding the captions.\n",
    "        mode (str): Mode of the data loader. Options: 'train' or 'valid'.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: Data loader for the specified mode.\n",
    "\n",
    "    \"\"\"\n",
    "    transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=2,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "class AvgMeter:\n",
    "    \"\"\"\n",
    "    Computes and tracks the average value of a metric.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the metric. Default is \"Metric\".\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the metric.\n",
    "        avg (float): Average value of the metric.\n",
    "        sum (float): Sum of the metric values.\n",
    "        count (int): Number of metric values.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        \"\"\"\n",
    "        Initializes an AvgMeter object.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the metric. Default is \"Metric\".\n",
    "\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the metric values to zero.\n",
    "\n",
    "        \"\"\"\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        \"\"\"\n",
    "        Updates the metric values with the given value.\n",
    "\n",
    "        Args:\n",
    "            val (float): Value to update the metric with.\n",
    "            count (int): Number of occurrences of the value. Default is 1.\n",
    "\n",
    "        \"\"\"\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the metric.\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of the metric in the format \"Name: Average\".\n",
    "\n",
    "        \"\"\"\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    \"\"\"\n",
    "    Get the learning rate of the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer: Optimizer object.\n",
    "\n",
    "    Returns:\n",
    "        float: Learning rate.\n",
    "\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_df, valid_df = make_train_valid_dfs()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode text input to a fixed-size vector representation.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the text encoder model. Default is \"distilbert-base-uncased\".\n",
    "        pretrained (bool): Whether to use pretrained weights for the model. Default is True.\n",
    "        trainable (bool): Whether to make the model trainable. Default is True.\n",
    "\n",
    "    Attributes:\n",
    "        model (transformers.DistilBertModel): Text encoder model.\n",
    "        target_token_idx (int): Index of the target token used for sentence embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=True):\n",
    "        \"\"\"\n",
    "        Initializes a TextEncoder object.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the text encoder model. Default is \"distilbert-base-uncased\".\n",
    "            pretrained (bool): Whether to use pretrained weights for the model. Default is True.\n",
    "            trainable (bool): Whether to make the model trainable. Default is True.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the TextEncoder.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask indicating which tokens to attend to.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded text representation.\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection head for projecting the input embeddings to a lower-dimensional space.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): Dimensionality of the input embeddings.\n",
    "        projection_dim (int): Dimensionality of the projected embeddings. Default is 256.\n",
    "        dropout (float): Dropout probability. Default is 0.\n",
    "\n",
    "    Attributes:\n",
    "        projection (torch.nn.Linear): Linear projection layer.\n",
    "        gelu (torch.nn.GELU): GELU activation function.\n",
    "        fc (torch.nn.Linear): Fully connected layer.\n",
    "        dropout (torch.nn.Dropout): Dropout layer.\n",
    "        layer_norm (torch.nn.LayerNorm): Layer normalization layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=128,\n",
    "        dropout=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a ProjectionHead object.\n",
    "\n",
    "        Args:\n",
    "            embedding_dim (int): Dimensionality of the input embeddings.\n",
    "            projection_dim (int): Dimensionality of the projected embeddings. Default is 256.\n",
    "            dropout (float): Dropout probability. Default is 0.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ProjectionHead.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings to be projected.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Projected embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive Language-Image Pretraining (CLIP) model.\n",
    "\n",
    "    Args:\n",
    "        temperature (float): Temperature parameter for the contrastive loss. Default is 1.\n",
    "        image_embedding (int): Dimensionality of the image embeddings. Default is 2048.\n",
    "        text_embedding (int): Dimensionality of the text embeddings. Default is 768.\n",
    "        projection_dim (int): Dimensionality of the projected embeddings. Default is 256.\n",
    "\n",
    "    Attributes:\n",
    "        image_encoder (ImageEncoder): Image encoder module.\n",
    "        text_encoder (TextEncoder): Text encoder module.\n",
    "        image_projection (ProjectionHead): Projection head for image embeddings.\n",
    "        text_projection (ProjectionHead): Projection head for text embeddings.\n",
    "        temperature (float): Temperature parameter for the contrastive loss.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=1,\n",
    "        image_embedding=2048,\n",
    "        text_embedding=768,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a CLIPModel object.\n",
    "\n",
    "        Args:\n",
    "            temperature (float): Temperature parameter for the contrastive loss. Default is 1.\n",
    "            image_embedding (int): Dimensionality of the image embeddings. Default is 2048.\n",
    "            text_embedding (int): Dimensionality of the text embeddings. Default is 768.\n",
    "            projection_dim (int): Dimensionality of the projected embeddings. Default is 256.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the CLIPModel.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Dictionary containing input data (image, input_ids, attention_mask).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean contrastive loss for the batch.\n",
    "\n",
    "        \"\"\"\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        batch_size = len(image_embeddings) \n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss between predicted logits and target probabilities.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted logits.\n",
    "        targets (torch.Tensor): Target probabilities.\n",
    "        reduction (str): Specifies the reduction to apply to the loss. \n",
    "                         Options: 'none' (no reduction), 'mean' (mean of the losses).\n",
    "                         Default is 'none'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Cross-entropy loss.\n",
    "\n",
    "    \"\"\"\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_embeddings(model, masks):\n",
    "    \"\"\"\n",
    "    Obtain embeddings for a list of masks using a given model.\n",
    "\n",
    "    Args:\n",
    "        model: The model used for generating embeddings.\n",
    "        masks (list): List of mask images.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated mask image embeddings.\n",
    "    \"\"\" \n",
    "    model.eval()\n",
    "    # Apply transforms\n",
    "    transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    \n",
    "    processed_masks = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        mask = np.array(mask)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = transforms(image=mask)['image']\n",
    "        mask = np.transpose(mask, (2, 0, 1))\n",
    "        processed_masks.append(mask)\n",
    "        \n",
    "    processed_masks = torch.tensor(processed_masks)\n",
    "\n",
    "    mask_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        image_features = model.image_encoder(processed_masks.to(device))\n",
    "        image_embeddings = model.image_projection(image_features)\n",
    "        mask_image_embeddings.append(image_embeddings)\n",
    "    return torch.cat(mask_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_masks(model, image_embeddings, query, n=9):\n",
    "    \"\"\"\n",
    "    Find matching masks based on a given query using image and text embeddings.\n",
    "\n",
    "    Args:\n",
    "        model: The model used for generating embeddings.\n",
    "        image_embeddings (torch.Tensor): Image embeddings.\n",
    "        query (str): The query used for matching.\n",
    "        n (int): Number of matching masks to retrieve (default: 9).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Dot product similarity matrix.\n",
    "        torch.Tensor: Top-k values of the similarity matrix.\n",
    "    \"\"\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(config.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), 1)\n",
    "    return dot_similarity, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIPModel().to(device)\n",
    "model.load_state_dict(torch.load('<path to trained CLIP weights>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_resnet = torchvision.datasets.ImageFolder(root='<path to evaluation dataset folder>', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_resnet.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet_model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_ftrs, len(dataset_resnet.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resnet_model.load_state_dict(torch.load('<path to saved trained resnet weigths eg:resnet1876.17260787992495.pth>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to be applied to each image\n",
    "resnet_transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "     transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # Normalize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokedex = pd.read_csv('<path to pokedex.csv file>')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code snippet demonstrates image classification and evaluation process. We start by iterating over a list of images and their corresponding annotations. For each image, we generate masks and filter out irrelevant annotations. We then extract cropped regions based on the filtered annotations and compute embeddings using a mask embedding model.\n",
    "\n",
    "Next, we process the class paths associated with the image and retrieve the correct labels. For each class, we calculate similarity scores between the mask image embeddings and the class label embeddings using the find_matching_masks function. We select the index of the highest score to obtain the predicted segment.\n",
    "\n",
    "To make a prediction, we transform the predicted segment and pass it through a pre-trained ResNet model. This gives us the predicted class label. We compare the predicted class label with the correct label, updating our accuracy metrics accordingly. Incorrectly classified segments are stored in a list called wrong. We print the accuracy after every 50 images and provide the final accuracy at the end of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scyther Scyther\n",
      "Hitmonlee Hitmonlee\n",
      "Fearow Fearow\n",
      "Venomoth Venomoth\n",
      "Grimer Grimer\n",
      "Voltorb Beedrill\n",
      "Voltorb Mew\n",
      "Weezing Weezing\n",
      "Krabby Krabby\n",
      "Slowbro Slowbro\n",
      "Pidgeotto Pidgeotto\n",
      "Jolteon Jolteon\n",
      "Psyduck Dodrio\n",
      "Kangaskhan Kangaskhan\n",
      "Psyduck Mr. Mime\n",
      "Weedle Weedle\n",
      "Hypno Zapdos\n",
      "Machop Machop\n",
      "Dratini Dratini\n",
      "Electrode Electrode\n",
      "Pinsir Pinsir\n",
      "Psyduck Abra\n",
      "Parasect Magmar\n",
      "Seaking Seaking\n",
      "Venusaur Venusaur\n",
      "Pidgey Pidgey\n",
      "Mewtwo Mewtwo\n",
      "Clefairy Clefairy\n",
      "Muk Cloyster\n",
      "Venusaur Venusaur\n",
      "Dewgong Kabutops\n",
      "Poliwhirl Poliwrath\n",
      "Seel Seel\n",
      "Growlithe Growlithe\n",
      "Primeape Primeape\n",
      "Seel Seel\n",
      "Raticate Raticate\n",
      "Charmander Doduo\n",
      "Raticate Seadra\n",
      "Psyduck Mr. Mime\n",
      "Psyduck Blastoise\n",
      "Charizard Charizard\n",
      "Weepinbell Weepinbell\n",
      "Rattata Rattata\n",
      "Nidorino Nidorino\n",
      "Kangaskhan Kangaskhan\n",
      "Golduck Golduck\n",
      "Hitmonchan Hitmonchan\n",
      "Lickitung Lickitung\n",
      "Parasect Parasect\n",
      "Accuracy after 50 images: 0.72\n",
      "Haunter Haunter\n",
      "Haunter Zapdos\n",
      "Weepinbell Tentacruel\n",
      "Psyduck Gastly\n",
      "Weepinbell Victreebel\n",
      "Rhydon Rhydon\n",
      "Clefairy Clefairy\n",
      "Ekans Ekans\n",
      "Dugtrio Dugtrio\n",
      "Psyduck Psyduck\n",
      "Clefable Clefable\n",
      "Articuno Articuno\n",
      "Staryu Staryu\n",
      "Wigglytuff Wigglytuff\n",
      "Abra Magmar\n",
      "Hypno Hypno\n",
      "Magnemite Magneton\n",
      "Dragonair Mew\n",
      "Rattata Rattata\n",
      "Blastoise Charizard\n",
      "Dewgong Dewgong\n",
      "Clefairy Clefairy\n",
      "Staryu Staryu\n",
      "Cloyster Cloyster\n",
      "Poliwhirl Poliwhirl\n",
      "Drowzee Drowzee\n",
      "Venomoth Venomoth\n",
      "Exeggcute Gloom\n",
      "Dratini Dratini\n",
      "Dewgong Kabutops\n",
      "Ponyta Ponyta\n",
      "Dewgong Dewgong\n",
      "Charmander Charmander\n",
      "Gyarados Gyarados\n",
      "Machamp Machamp\n",
      "Sandslash Kingler\n",
      "Psyduck Magmar\n",
      "Cubone Cubone\n",
      "Moltres Pidgey\n",
      "Dewgong Kabutops\n",
      "Charmander Eevee\n",
      "Poliwhirl Poliwhirl\n",
      "Shellder Shellder\n",
      "Dratini Dratini\n",
      "Ekans Ekans\n",
      "Ivysaur Ivysaur\n",
      "Psyduck Gastly\n",
      "Voltorb Blastoise\n",
      "Parasect Parasect\n",
      "Parasect Tentacruel\n",
      "Accuracy after 100 images: 0.68\n",
      "Omastar Nidoran♂\n",
      "Tauros Tauros\n",
      "Omastar Omastar\n",
      "Golbat Zubat\n",
      "Alakazam Gyarados\n",
      "Flareon Flareon\n",
      "Rhyhorn Rhyhorn\n",
      "Psyduck Pikachu\n",
      "Rhydon Rhydon\n",
      "Charizard Charizard\n",
      "Oddish Oddish\n",
      "Fearow Fearow\n",
      "Kabuto Kabuto\n",
      "Horsea Caterpie\n",
      "Horsea Horsea\n",
      "Pikachu Pikachu\n",
      "Bulbasaur Magneton\n",
      "Slowpoke Slowpoke\n",
      "Cloyster Cloyster\n",
      "Magikarp Magikarp\n",
      "Voltorb Mew\n",
      "Charmander Charmander\n",
      "Omastar Omastar\n",
      "Metapod Metapod\n",
      "Kabuto Kabuto\n",
      "Psyduck Primeape\n",
      "Psyduck Krabby\n",
      "Mewtwo Gengar\n",
      "Hitmonchan Hitmonchan\n",
      "Dragonair Dragonair\n",
      "Gastly Gastly\n",
      "Golbat Golbat\n",
      "Wartortle Wartortle\n",
      "Shellder Shellder\n",
      "Pikachu Pikachu\n",
      "Lapras Lapras\n",
      "Staryu Staryu\n",
      "Pikachu Kadabra\n",
      "Machamp Machamp\n",
      "Chansey Chansey\n",
      "Dewgong Dewgong\n",
      "Metapod Farfetch'd\n",
      "Venusaur Venusaur\n",
      "Electrode Electrode\n",
      "Squirtle Squirtle\n",
      "Charmander Charmander\n",
      "Kangaskhan Kangaskhan\n",
      "Horsea Horsea\n",
      "Dugtrio Snorlax\n",
      "Jolteon Jolteon\n",
      "Accuracy after 150 images: 0.7\n",
      "Squirtle Arbok\n",
      "Dugtrio Dugtrio\n",
      "Golbat Mr. Mime\n",
      "Haunter Haunter\n",
      "Clefairy Clefairy\n",
      "Cloyster Cloyster\n",
      "Nidorino Nidorino\n",
      "Zubat Zubat\n",
      "Rhydon Rhydon\n",
      "Mewtwo Nidoran♀\n",
      "Wigglytuff Wigglytuff\n",
      "Wigglytuff Eevee\n",
      "Lapras Lapras\n",
      "Squirtle Squirtle\n",
      "Beedrill Beedrill\n",
      "Persian Rapidash\n",
      "Bulbasaur Bulbasaur\n",
      "Dewgong Kabutops\n",
      "Onix Onix\n",
      "Voltorb Mew\n",
      "Mewtwo Mewtwo\n",
      "Oddish Oddish\n",
      "Hitmonlee Hitmonlee\n",
      "Dratini Muk\n",
      "Flareon Flareon\n",
      "Pikachu Kadabra\n",
      "Weezing Weezing\n",
      "Poliwhirl Poliwhirl\n",
      "Voltorb Mewtwo\n",
      "Jigglypuff Jigglypuff\n",
      "Pikachu Pikachu\n",
      "Bulbasaur Bulbasaur\n",
      "Dratini Dratini\n",
      "Flareon Flareon\n",
      "Beedrill Beedrill\n",
      "Beedrill Butterfree\n",
      "Psyduck Raticate\n",
      "Cloyster Cloyster\n",
      "Pidgeot Pidgeot\n",
      "Psyduck Kangaskhan\n",
      "Weepinbell Weepinbell\n",
      "Golem Golem\n",
      "Cubone Cubone\n",
      "Horsea Horsea\n",
      "Magikarp Magikarp\n",
      "Dewgong Mewtwo\n",
      "Dratini Dratini\n",
      "Golbat Charizard\n",
      "Psyduck Tentacruel\n",
      "Seaking Seaking\n",
      "Accuracy after 200 images: 0.695\n",
      "Weezing Weezing\n",
      "Gloom Gloom\n",
      "Rhyhorn Rhyhorn\n",
      "Voltorb Beedrill\n",
      "Weezing Weezing\n",
      "Gyarados Gyarados\n",
      "Machamp Machamp\n",
      "Rattata Rattata\n",
      "Mewtwo Mewtwo\n",
      "Squirtle Squirtle\n",
      "Drowzee Drowzee\n",
      "Voltorb Voltorb\n",
      "Rapidash Rapidash\n",
      "Snorlax Snorlax\n",
      "Kabuto Marowak\n",
      "Kabuto Blastoise\n",
      "Rapidash Rapidash\n",
      "Parasect Parasect\n",
      "Starmie Starmie\n",
      "Drowzee Drowzee\n",
      "Poliwrath Poliwrath\n",
      "Hypno Electabuzz\n",
      "Kakuna Exeggcute\n",
      "Paras Paras\n",
      "Mewtwo Mewtwo\n",
      "Wigglytuff Wigglytuff\n",
      "Dewgong Beedrill\n",
      "Raichu Raichu\n",
      "Snorlax Snorlax\n",
      "Metapod Marowak\n",
      "Magnemite Seel\n",
      "Metapod Pikachu\n",
      "Tentacruel Tentacruel\n",
      "Dewgong Dewgong\n",
      "Kangaskhan Kangaskhan\n",
      "Cubone Cubone\n",
      "Seadra Seadra\n",
      "Dewgong Magneton\n",
      "Pikachu Pikachu\n",
      "Omanyte Omanyte\n",
      "Magnemite Magnemite\n",
      "Dugtrio Dugtrio\n",
      "Golbat Tentacruel\n",
      "Pidgeot Pidgeotto\n",
      "Rapidash Rapidash\n",
      "Haunter Haunter\n",
      "Jolteon Jolteon\n",
      "Poliwhirl Poliwhirl\n",
      "Magnemite Magnemite\n",
      "Sandshrew Sandshrew\n",
      "Accuracy after 250 images: 0.708\n",
      "Golbat Golbat\n",
      "Haunter Haunter\n",
      "Aerodactyl Aerodactyl\n",
      "Psyduck Venusaur\n",
      "Kangaskhan Kangaskhan\n",
      "Onix Pinsir\n",
      "Bulbasaur Bulbasaur\n",
      "Hypno Hypno\n",
      "Tangela Tangela\n",
      "Jolteon Jolteon\n",
      "Venomoth Venomoth\n",
      "Scyther Exeggutor\n",
      "Pikachu Pikachu\n",
      "Moltres Alakazam\n",
      "Nidorina Nidoran♀\n",
      "Exeggutor Exeggutor\n",
      "Porygon Porygon\n",
      "Hypno Hypno\n",
      "Dewgong Porygon\n",
      "Zubat Zubat\n",
      "Persian Persian\n",
      "Golduck Golduck\n",
      "Geodude Geodude\n",
      "Raichu Pikachu\n",
      "Electrode Electrode\n",
      "Nidorino Nidorino\n",
      "Golduck Golduck\n",
      "Aerodactyl Aerodactyl\n",
      "Tauros Tauros\n",
      "Vileplume Vileplume\n",
      "Pinsir Pinsir\n",
      "Kingler Kingler\n",
      "Weezing Weezing\n",
      "Graveler Graveler\n",
      "Poliwhirl Poliwhirl\n",
      "Kabuto Beedrill\n",
      "Dratini Dratini\n",
      "Metapod Marowak\n",
      "Arcanine Arcanine\n",
      "Magnemite Blastoise\n",
      "Golduck Golduck\n",
      "Golduck Charizard\n",
      "Kabuto Kabuto\n",
      "Golduck Golduck\n",
      "Exeggutor Exeggutor\n",
      "Dragonair Magneton\n",
      "Sandshrew Sandshrew\n",
      "Weepinbell Weepinbell\n",
      "Mewtwo Tentacruel\n",
      "Tauros Tauros\n",
      "Accuracy after 300 images: 0.7133333333333334\n",
      "Rattata Rattata\n",
      "Dewgong Pinsir\n",
      "Dewgong Nidoran♂\n",
      "Raticate Raticate\n",
      "Wartortle Wartortle\n",
      "Psyduck Nidoran♂\n",
      "Clefairy Clefairy\n",
      "Psyduck Magmar\n",
      "Spearow Spearow\n",
      "Golbat Magikarp\n",
      "Geodude Geodude\n",
      "Aerodactyl Gyarados\n",
      "Golduck Golduck\n",
      "Ponyta Rapidash\n",
      "Magikarp Magikarp\n",
      "Pidgeot Pidgeotto\n",
      "Poliwhirl Poliwhirl\n",
      "Exeggutor Exeggutor\n",
      "Weepinbell Weepinbell\n",
      "Poliwhirl Nidoran♂\n",
      "Vileplume Vileplume\n",
      "Sandshrew Sandshrew\n",
      "Pikachu Pikachu\n",
      "Raichu Raichu\n",
      "Magnemite Weedle\n",
      "Dewgong Pikachu\n",
      "Geodude Geodude\n",
      "Mankey Mr. Mime\n",
      "Dewgong Venusaur\n",
      "Bellsprout Bellsprout\n",
      "Dewgong Hitmonchan\n",
      "Rapidash Rapidash\n",
      "Hypno Hypno\n",
      "Weezing Weezing\n",
      "Weezing Weezing\n",
      "Mewtwo Magneton\n",
      "Voltorb Voltorb\n",
      "Poliwag Poliwag\n",
      "Electrode Krabby\n",
      "Tentacool Tentacool\n",
      "Bulbasaur Ivysaur\n",
      "Poliwag Poliwag\n",
      "Voltorb Blastoise\n",
      "Dewgong Pinsir\n",
      "Dratini Dratini\n",
      "Dewgong Dewgong\n",
      "Persian Venusaur\n",
      "Seadra Seadra\n",
      "Oddish Oddish\n",
      "Dewgong Magneton\n",
      "Accuracy after 350 images: 0.6942857142857143\n",
      "Rhydon Rhydon\n",
      "Dewgong Dewgong\n",
      "Moltres Moltres\n",
      "Ninetales Ninetales\n",
      "Electrode Electrode\n",
      "Shellder Shellder\n",
      "Shellder Shellder\n",
      "Dewgong Kadabra\n",
      "Staryu Staryu\n",
      "Omanyte Omanyte\n",
      "Fearow Dragonite\n",
      "Machop Machop\n",
      "Raichu Raichu\n",
      "Raticate Raticate\n",
      "Fearow Magmar\n",
      "Jigglypuff Jigglypuff\n",
      "Jolteon Jolteon\n",
      "Graveler Graveler\n",
      "Ninetales Ninetales\n",
      "Growlithe Magmar\n",
      "Cubone Golduck\n",
      "Cubone Cubone\n",
      "Clefable Clefable\n",
      "Kingler Kingler\n",
      "Bulbasaur Bulbasaur\n",
      "Raticate Raticate\n",
      "Omastar Nidoran♂\n",
      "Omastar Omastar\n",
      "Voltorb Blastoise\n",
      "Omastar Gyarados\n",
      "Weepinbell Sandshrew\n",
      "Psyduck Psyduck\n",
      "Metapod Marowak\n",
      "Tentacool Tentacool\n",
      "Ponyta Ponyta\n",
      "Cubone Cubone\n",
      "Cubone Kadabra\n",
      "Arbok Arbok\n",
      "Dewgong Kadabra\n",
      "Mewtwo Mewtwo\n",
      "Snorlax Snorlax\n",
      "Snorlax Nidoran♂\n",
      "Dugtrio Dugtrio\n",
      "Haunter Haunter\n",
      "Ditto Ditto\n",
      "Pidgeot Pidgeot\n",
      "Exeggcute Exeggcute\n",
      "Kangaskhan Kangaskhan\n",
      "Staryu Staryu\n",
      "Psyduck Gastly\n",
      "Accuracy after 400 images: 0.6975\n",
      "Final accuracy: 0.6975\n"
     ]
    }
   ],
   "source": [
    "images = df['images'].tolist()\n",
    "class_list = df['classes'].tolist()\n",
    "correct = 0\n",
    "wrong = []\n",
    "counter = 0\n",
    "for i, image in enumerate(images):\n",
    "    \n",
    "    image_path = dataset_path + '/' + image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    masks = mask_generator.generate(img)\n",
    "    annotations =  masks\n",
    "    new_annotaions = filter_annotations(annotations)\n",
    "    new_annotaions.pop(0)\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    cropped_boxes = []\n",
    "\n",
    "    for mask in new_annotaions:\n",
    "        cropped_boxes.append(segment_image(image, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))\n",
    "    \n",
    "    mask_image_embeddings = get_masks_embeddings(model, cropped_boxes)\n",
    "\n",
    "    classes_paths = class_list[i].split('_')\n",
    "    classes_paths.pop()\n",
    "    classes = []\n",
    "    for c in classes_paths:\n",
    "        index = c.split('/')[-1]\n",
    "        if \"-\" in index:\n",
    "            index = index.split('-')[0]\n",
    "        index = index.split('.')[0]\n",
    "        correct_label = list(pokedex[pokedex['Name'] == int(index)]['Type 1'])[0]\n",
    "        classes.append(correct_label)\n",
    "    for cl in classes:\n",
    "        scores, values = find_matching_masks(model, mask_image_embeddings, cl)\n",
    "        max_idx = torch.argmax(scores)\n",
    "        predicted_segment = cropped_boxes[max_idx]\n",
    "\n",
    "        transformed_segment =  resnet_transform(predicted_segment)\n",
    "        transformed_segment =  transformed_segment.view(1, 3, 224, 224)\n",
    "        resnet_model.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_resnet_label = resnet_model(transformed_segment)\n",
    "            predicted_resnet_label = torch.argmax(predicted_resnet_label)\n",
    "            predicted_class = dataset_resnet.classes[predicted_resnet_label]\n",
    "        print(predicted_class, cl)\n",
    "        if predicted_class.lower() == cl.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong.append(predicted_segment)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 50 == 0:\n",
    "            accuracy = correct / counter\n",
    "            print(f\"Accuracy after {counter} images: {accuracy}\")\n",
    "print(f\"Final accuracy: {correct / counter}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "de935e14300db630c8585086d88fa64e33aea992cc54f5441c580cc68c8bfe48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
